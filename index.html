<!DOCTYPE html>

<html>

<head lang="en">
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>UDE-2</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" href="./assets/bootstrap.min.css">
    <link rel="stylesheet" href="./assets/font-awesome.min.css">
    <link rel="stylesheet" href="./assets/codemirror.min.css">
    <link rel="stylesheet" href="./assets/app.css">




</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h1 class="col-md-20 text-center">
                <br></br>
                <b>A Unified Framework for Multimodal, Multi-Part Human Motion Synthesis<br>
<!--                 <small>
                    CVPR 2022 (Oral Presentation)
                </small> -->
            </h1>
            <hr style="margin-top:0px">
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://zixiangzhou916.github.io/" style="font-size: 16px;">
                            Zixiang Zhou
                        </a>
                        <!-- <sup>1</sup> -->
                    </li>
                    <li>
                        <a href="wanyu@xiaobing.ai" style="font-size: 16px;">
                            Yu Wan
                        </a>
                        <!-- <sup>1</sup> -->
                    </li>

                    <li>
                        <a href="https://sites.google.com/site/zjuwby/?pli=1" style="font-size: 16px;">
                            Baoyuan Wang
                        </a>
                        <!-- <sup>1</sup> -->
                    </li><br>
                    <a></a><br>
                    <li>
                        <!-- <sup>1</sup> -->
                        <a href="https://www.xiaoice.com/" style="font-size: 16px;">
                            <img src="./assets/xiaobing.png" height="20px">
                            Xiaobing.ai
                        </a>
                    </li>
                </ul>
            </div>
        </div>


        <div class="row">
            <div class="col-md-4 col-md-offset-4 text-center">
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <!-- <a href="http://arxiv.org/abs/2211.16016"> -->
                        <a>
                            <img src="./assets/arxiv.png" height="60px">
                            <h4><strong>Paper</strong></h4>
                        </a>
                    </li>

                    <li>
<!--                         <a onClick="alert('Code coming soon!\nContact dengyu2008@hotmail.com for more details.')"> -->
                        <!-- <a href="https://github.com/dorniwang/PD-FGC"> -->
                        <a href="https://github.com/zixiangzhou916/UDE-2">
                            <img src="./assets/github.png" height="60px">
                            <h4><strong>Code (coming soon)</strong></h4>
                        </a>
                    </li>
                </ul>
            </div>
        </div>

        <div class="row">
            <div class="col-md-12 col-md-offset-0 text-center">
                <a>
                    <!-- <video style="width:100%;height:100%;" playsinline autoplay loop preload muted>
                        <source src="./files/cover.mp4" type="video/mp4">
                    </video> -->
                    <img src="./assets/teaser.png" class="img-responsive" alt="teaser"><br>
                    
                </a>
                <p class="text-justify" style="font-size: 16px;">
                    In this work, we propose a unified human motion generation model that accepts three distinct modalities, and enables the synthesis of diverse body part motions.
                </p>
                <br></br>
                <h2>
                    Abstract
                </h2>
                <hr style="margin-top:0px">
                <p class="text-justify" style="font-size: 16px;">
                    The field has made significant progress in synthesizing realistic human motion driven by various modalities. Yet, the need for different methods to animate various body parts according to different control signals limits the scalability of these techniques in practical scenarios. In this paper, we introduce a cohesive and scalable approach that consolidates multimodal (text, music, speech) and multi-part (hand, torso) human motion generation. Our methodology unfolds in several steps: We begin by quantizing the motions of diverse body parts into separate codebooks tailored to their respective domains. Next, we harness the robust capabilities of pre-trained models to transcode multimodal signals into a shared latent space. We then translate these signals into discrete motion tokens by iteratively predicting subsequent tokens to form a complete sequence. Finally, we reconstruct the continuous actual motion from this tokenized sequence. Our method frames the multimodal motion generation challenge as a token prediction task, drawing from specialized codebooks based on the modality of the control signal. This approach is inherently scalable, allowing for the easy integration of new modalities. Extensive experiments demonstrated the effectiveness of our design, emphasizing its potential for broad application.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-12 col-md-offset-0 text-center">
                <br></br>
                <h2>
                    Video
                </h2>
                <hr style="margin-top:0px">
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="https://www.youtube.com/embed/xrkIrKnSrBo?si=Gi28q8PjigGRZphC"" allowfullscreen=""
                            style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-12 col-md-offset-0 text-center">
                <br></br>
                <h2>
                    Overview
                </h2>
                <hr style="margin-top:0px">
                <img src="./assets/pipeline.png" class="img-responsive" alt="overview"><br>
                <p class="text-justify" style="font-size: 20px;">
                    The overview of our method. 
                </p>
                <p class="text-justify" style="font-size: 16px;">
                    Hierarchical torso VQ-VAE:
                    <p class="text-justify" style="font-size: 14px;">
                        1. Encode the relative trajectory instead of global trajectory.
                    </p>
                    <p class="text-justify" style="font-size: 14px;">
                        2. Decode the discrete tokens to local poses(1st stage).
                    </p>
                    <p class="text-justify" style="font-size: 14px;">
                        3. Estimate sub-optimal global poses.
                    </p>
                    <p class="text-justify" style="font-size: 14px;">
                        4. Refine to obtain the final global poses(2nd stage).
                    </p>
                    <p class="text-justify" style="font-size: 14px;">
                        5. Introduce weights re-initialization to improve tokens activation rate.
                    </p>
                </p>
                <p class="text-justify" style="font-size: 16px;">
                    Multimodal Multi-Part Motion Generation
                    <p class="text-justify" style="font-size: 14px;">
                        1. Use large-scale pretrained models as encoders: 1) CLIP text encoder, 2) MTR, 3) HuBERT.
                    </p>
                    <p class="text-justify" style="font-size: 14px;">
                        2. Use encoder-decoder architecture to transform multimodal condition to motion tokens.
                    </p>
                    <p class="text-justify" style="font-size: 14px;">
                        3. Accept auxiliary condition as learnable embeddings.
                    </p>
                    <p class="text-justify" style="font-size: 14px;">
                        4. Introduce semantic enhancement module to align semantic between condition and motion.
                    </p>
                    <p class="text-justify" style="font-size: 14px;">
                        5. Introduce semantic-aware sampling to improve condition consistency while maintaining synthesis diversity.
                    </p>
                    
                </p>
            </div>
        </div>


        <div class="row">
            <div class="col-md-12 col-md-offset-0 text-center">
                <br></br>
                <h2>
                    More Results
                </h2>
                <hr style="margin-top:0px">
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="https://www.youtube.com/embed/ajXeJzruOEQ?si=NxJ3YsAGvhBzqVmL"" allowfullscreen=""
                            style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="https://www.youtube.com/embed/69_r-N_EANc?si=Exapv15xopXlRr6D"" allowfullscreen=""
                            style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="https://www.youtube.com/embed/xvvVLck79mE?si=7GY-RjMTUHCScw6p"" allowfullscreen=""
                            style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div>


        <div class="row">
            <div class="col-md-12 col-md-offset-0 text-center">
                <br></br>
                <h2>
                    Acknowledgements
                </h2>
                <hr style="margin-top:0px">
                <p class="text-justify" style="font-size: 16px;">
                    <!-- We thank Harry Shum for the fruitful advice and discussion to improve the paper. <br> -->
                    The website template was adapted from <a href="https://yudeng.github.io/GRAM/">GRAM</a>.
                </p>
            </div>
        </div>


</body>

</html>